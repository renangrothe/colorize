{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34976832-775b-4129-a360-c6d36cb46ad1",
   "metadata": {},
   "source": [
    "# Colorização Automática de Imagens em Tons de Cinza via Classificação Multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3029-973a-4aab-8362-76b3fe99684a",
   "metadata": {},
   "source": [
    "#### O objetivo deste notebook é implementar um algoritmo de Deep Learning para a disciplina de Processamento de Imagens, seguindo a referência e embasamento teórico encontrado em http://richzhang.github.io/colorization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd30f7-0611-43cd-a292-3665b81b4236",
   "metadata": {},
   "source": [
    "### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a96d74a-55d6-4cf0-be2c-0df4042737de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from skimage import color\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18466d88-4f02-4e8d-8e10-557111bda5e6",
   "metadata": {},
   "source": [
    "### Configurações de execução\n",
    "#### 'Q' representa o número de 'intervalos discretos' de categorias (cores) disponíveis para a classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf826b7-496a-446e-a162-69ccb2a5d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else None\n",
    "BATCH_SIZE = 16 \n",
    "IMG_SIZE = 256\n",
    "Q = 313  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac3a30-f48b-4da3-9975-f950f1385a02",
   "metadata": {},
   "source": [
    "### Pré-processamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d076a77-1e69-43f8-8edd-60d81ba3fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogColorizationDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root = root_dir\n",
    "        try:\n",
    "            self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir) \n",
    "                           if f.endswith(('.JPEG'))]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "        # Carregar bins AB pré-processados (em execuções anteriores)\n",
    "        self.ab_bins = torch.from_numpy(np.load('data/ab_bins.npy')).float()\n",
    "        self.weights = torch.load('data/color_weights.pt')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        rgb_img = self.transform(rgb_img)\n",
    "        \n",
    "        # Converter para Lab e normalizar\n",
    "        lab_img = color.rgb2lab(np.array(rgb_img)).astype(np.float32)\n",
    "        L = lab_img[:,:,0] / 50.0 - 1.0  # [-1, 1]\n",
    "        ab = lab_img[:,:,1:] / 128.0      # [-1, 1]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(L).unsqueeze(0).to(DEVICE),  # L channel\n",
    "            torch.FloatTensor(ab).permute(2,0,1).to(DEVICE) # AB channels\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ccdfd-da83-4001-838e-5e3f0d3a2211",
   "metadata": {},
   "source": [
    "### Arquitetura do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea92368-6340-4c62-bbb3-48fbc30088d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (Downsampling)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        # Middle (Dilated Convs)\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        \n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Conv2d(64, Q, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)    # 128x128\n",
    "        x = self.enc2(x)    # 64x64\n",
    "        x = self.enc3(x)    # 32x32\n",
    "        x = self.mid(x)     # 32x32\n",
    "        x = self.dec1(x)    # 64x64\n",
    "        x = self.dec2(x)    # 128x128\n",
    "        x = self.dec3(x)    # 256x256\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d3c7a-5600-4f83-9148-2b25cbcc5742",
   "metadata": {},
   "source": [
    "### Função de perda aplicada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d6bd09-fb65-4496-a48f-bd9b119ff17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ab_bins = torch.from_numpy(np.load('data/ab_bins.npy')).float().to(DEVICE)\n",
    "        self.weights = torch.load('data/color_weights.pt').to(DEVICE)\n",
    "    \n",
    "    def soft_encode(self, ab):\n",
    "        # Encontrar 5 vizinhos mais próximos\n",
    "        ab_flat = ab.permute(0,2,3,1).reshape(-1, 2)\n",
    "        dists = torch.cdist(ab_flat, self.ab_bins)\n",
    "        _, top5 = torch.topk(dists, 5, largest=False, dim=1)\n",
    "        \n",
    "        # Suavização Gaussiana\n",
    "        sigma = 5.0\n",
    "        weights = torch.exp(-dists.gather(1, top5)**2/(2*sigma**2))\n",
    "        weights = weights / weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Codificação suave\n",
    "        soft_labels = torch.zeros(ab_flat.size(0), Q, device=DEVICE)\n",
    "        soft_labels.scatter_(1, top5, weights)\n",
    "        return soft_labels.view(ab.size(0), IMG_SIZE, IMG_SIZE, Q).permute(0,3,1,2)\n",
    "\n",
    "    def forward(self, pred, ab_true):\n",
    "        soft_targets = self.soft_encode(ab_true*128)  # Desnormalizar\n",
    "        loss = F.cross_entropy(pred, soft_targets.argmax(dim=1), \n",
    "                              weight=self.weights, reduction='mean')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301594dd-d05f-4ffd-aa37-33ddc7017819",
   "metadata": {},
   "source": [
    "### Pré-processar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b71d5f-3d4c-4da2-9e44-067ba86738e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/color_weights.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     torch.save(torch.FloatTensor(weights), \u001b[33m'\u001b[39m\u001b[33mdata/color_weights.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Executar pré-processamento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mpreprocess_colors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mpreprocess_colors\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m np.save(\u001b[33m'\u001b[39m\u001b[33mdata/ab_bins.npy\u001b[39m\u001b[33m'\u001b[39m, ab_bins_valid)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 3. Calcular distribuição de pesos (usando Imagewoof)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m train_dataset = \u001b[43mDogColorizationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Usar dataset real\u001b[39;00m\n\u001b[32m     27\u001b[39m hist = np.zeros(\u001b[38;5;28mlen\u001b[39m(ab_bins_valid))\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m L, ab \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mDogColorizationDataset.__init__\u001b[39m\u001b[34m(self, root_dir)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Carregar bins AB pré-processados (em execuções anteriores)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.ab_bins = torch.from_numpy(np.load(\u001b[33m'\u001b[39m\u001b[33mdata/ab_bins.npy\u001b[39m\u001b[33m'\u001b[39m)).float()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mself\u001b[39m.weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/color_weights.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/color_weights.pt'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess_colors():\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # 1. Definir bins AB (grid size 10 como no artigo)\n",
    "    q = 10\n",
    "    ab_bins = np.linspace(-128, 128, q*2+1)\n",
    "    ab_grid = np.meshgrid(ab_bins, ab_bins)\n",
    "    ab_bins = np.stack(ab_grid, axis=-1).reshape(-1, 2)\n",
    "    \n",
    "    # 2. Filtrar bins inválidos (fora da gama RGB)\n",
    "    valid_bins = []\n",
    "    for a, b in ab_bins:\n",
    "        # Converter Lab para RGB\n",
    "        lab = np.array([[50, a, b]], dtype=np.float32)  # L=50 (valor neutro)\n",
    "        rgb = color.lab2rgb(lab)[0,0]\n",
    "        if (rgb >= 0).all() and (rgb <= 1).all():\n",
    "            valid_bins.append([a, b])\n",
    "    \n",
    "    ab_bins_valid = np.array(valid_bins)\n",
    "    np.save('data/ab_bins.npy', ab_bins_valid)\n",
    "    \n",
    "    # 3. Calcular distribuição de pesos (usando Imagewoof)\n",
    "    train_dataset = DogColorizationDataset('data/train')  # Usar dataset real\n",
    "    hist = np.zeros(len(ab_bins_valid))\n",
    "    \n",
    "    for L, ab in train_dataset:\n",
    "        ab_flat = ab.squeeze().permute(1,2,0).cpu().numpy() * 128  # Desnormalizar\n",
    "        dists = np.linalg.norm(ab_flat[:, :, None] - ab_bins_valid, axis=-1)\n",
    "        nearest = np.argmin(dists, axis=-1)\n",
    "        hist += np.bincount(nearest.flatten(), minlength=len(ab_bins_valid))\n",
    "    \n",
    "    # Suavizar e calcular pesos\n",
    "    sigma = 5\n",
    "    p = hist / hist.sum()\n",
    "    p_smooth = np.zeros_like(p)\n",
    "    for i in range(len(ab_bins_valid)):\n",
    "        dists = np.linalg.norm(ab_bins_valid - ab_bins_valid[i], axis=-1)\n",
    "        p_smooth[i] = np.sum(p * np.exp(-dists**2/(2*sigma**2)))\n",
    "    \n",
    "    p_smooth = p_smooth / p_smooth.sum()\n",
    "    weights = 1 / ((1 - 0.5) * p_smooth + 0.5 / len(ab_bins_valid))\n",
    "    weights = weights / weights.mean()  # Normalizar\n",
    "    torch.save(torch.FloatTensor(weights), 'data/color_weights.pt')\n",
    "\n",
    "# Executar pré-processamento\n",
    "preprocess_colors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce6c51-4c1d-422c-9dba-559c71b93a3d",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7400c0a4-b1d3-4f39-bad6-a290051e65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def train(resume_checkpoint=None):\n",
    "    train_set = DogColorizationDataset('data/train')\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                             num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = ColorNet().to(DEVICE)\n",
    "    criterion = ColorLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Variáveis de estado do treinamento\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    loss_history = []\n",
    "\n",
    "    # Carregar checkpoint se especificado\n",
    "    if resume_checkpoint:\n",
    "        checkpoint = torch.load(resume_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        loss_history = checkpoint['loss_history']\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, 100):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for i, (L, ab) in enumerate(train_loader):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(L)\n",
    "                loss = criterion(pred, ab)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Salvamento intermediário a cada 25% de uma época\n",
    "            if i % (len(train_loader)//4) == 0:\n",
    "                checkpoint_path = os.path.join(CHECKPOINT_DIR, \n",
    "                    f'interim_epoch_{epoch}_batch_{i}.pt')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': i,\n",
    "                    'model_state': model.state_dict(),\n",
    "                    'optimizer_state': optimizer.state_dict(),\n",
    "                    'scaler_state': scaler.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    'best_loss': best_loss,\n",
    "                    'loss_history': loss_history\n",
    "                }, checkpoint_path)\n",
    "        \n",
    "        # Salvamento completo no final de cada época\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        # Salvar checkpoint regular\n",
    "        regular_checkpoint = os.path.join(CHECKPOINT_DIR, f'epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scaler_state': scaler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'best_loss': best_loss,\n",
    "            'loss_history': loss_history\n",
    "        }, regular_checkpoint)\n",
    "        \n",
    "        # Salvar melhor modelo\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_model.pt'))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}, Best Loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22c7ae4-7402-4fdc-8716-15df70dd71a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ab_bins.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     train(resume_checkpoint=latest_checkpoint)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(resume_checkpoint)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(resume_checkpoint=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     train_set = \u001b[43mDogColorizationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m                              num_workers=\u001b[32m4\u001b[39m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m     model = ColorNet().to(DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mDogColorizationDataset.__init__\u001b[39m\u001b[34m(self, root_dir)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = transforms.Compose([\n\u001b[32m      8\u001b[39m     transforms.Resize((IMG_SIZE, IMG_SIZE)),\n\u001b[32m      9\u001b[39m     transforms.RandomHorizontalFlip(),\n\u001b[32m     10\u001b[39m ])\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Carregar bins AB pré-processados (em execuções anteriores)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mself\u001b[39m.ab_bins = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/ab_bins.npy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m).float()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.weights = torch.load(\u001b[33m'\u001b[39m\u001b[33mdata/color_weights.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    449\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/ab_bins.npy'"
     ]
    }
   ],
   "source": [
    "def find_latest_checkpoint():\n",
    "    checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('epoch_')]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    latest = sorted(checkpoints, key=lambda x: int(x.split('_')[1].split('.')[0]))[-1]\n",
    "    return os.path.join(CHECKPOINT_DIR, latest)\n",
    "\n",
    "# Modifique a execução principal para resumir automaticamente\n",
    "if __name__ == '__main__':\n",
    "    latest_checkpoint = find_latest_checkpoint()\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "        train(resume_checkpoint=latest_checkpoint)\n",
    "    else:\n",
    "        train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312857dd-d52a-4bea-aabe-879b051a29d2",
   "metadata": {},
   "source": [
    "### Inferir (1 imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f86b6-9581-4e2d-a611-4c03844e425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(image_path, model_path='color_net.pth'):\n",
    "    model = ColorNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Pré-processamento\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = transforms.Resize((IMG_SIZE, IMG_SIZE))(img)\n",
    "    L = color.rgb2lab(np.array(img))[:,:,0]\n",
    "    L_tensor = torch.FloatTensor(L/50.0 - 1.0).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Predição\n",
    "    with torch.no_grad():\n",
    "        pred = model(L_tensor)\n",
    "        probs = F.softmax(pred/0.38, dim=1)  # Annealed mean\n",
    "        ab = torch.einsum('bqhw,qc->bchw', probs, self.ab_bins)\n",
    "    \n",
    "    # Pós-processamento\n",
    "    lab = torch.cat([(L_tensor.squeeze()*50 + 50).unsqueeze(0), ab.squeeze()*128], dim=0)\n",
    "    rgb = color.lab2rgb(lab.permute(1,2,0).cpu().numpy())\n",
    "    \n",
    "    plt.imshow(rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
